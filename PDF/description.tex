\documentclass[a4paper, 11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[onehalfspacing]{setspace}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\setcounter{tocdepth}{4}
\usepackage{algorithmicx}
\usepackage{subfigure}
\usepackage{printlen}
\usepackage[labelfont={bf,sf},font={scriptsize},%
  labelsep=colon]{caption}
\usepackage{hyperref}
\usepackage[backend=bibtex]{biblatex}

\title{Constructing a Compiler}

\author{Daniel Pietsch}

\date{22. Januar 2023}


\begin{document}


\maketitle
\tableofcontents
\newpage

\section{Abstract}
The goal of this work is to describe the fundamental parts of a compiler and to give a brief discription of a self written interpreter, which was created as a final test of the course Compiler Construction during winter term 2022/2023.\\
Compilers are programs that take a source code written in some programming language as input and translate it into machine code so that a specific CPU can run it.
To do this more easily one can split the compilation process into some steps, where each step can be approached on it's own using the output of the previous step mostly.\\

\section{Compilation Steps}
In the following section a step by step approach of an interpretation of a source file is shown. There will be a discription of the used tools like flex and bison, which were shown during the course of the lecture and taught during the practical lessons of the course. Since my programming language is interpreted, there is no machine code generated, but all actions are executed during traversal of the abstract syntax tree, which in turn is created during the parse step.
\subsection{Lexical Analysis}
The first of those steps is the Lexical Analysis. A Lexer or Lexical Analyst gets a character stream (the source code) and "tokenizes" it into Tokens (tokenizer) that are groups of those characters that have a specific meaning for a program. That's why a lot of people also call the Lexer a Scanner. A token or lexeme can be anything from keywords, operators to identifiers or even constant/literals. The Tokens form the so called Lexical Grammar of that programming language. The actual lexer/scanner is a Finite-State-Machine. The tool of choice that lets one generate all the input tokens is called Flex. It is an open source alternative to Lex and used to create lexical analyzers.
\subsection{Syntax analyzing or parsing}
The second step is to parse the tokens generated by the Lexer. It is done by parsing the tokens according to a grammar, specified to meet the needs of the desired programming language. A tool that helps creating those parsers is the free and open-source program Bison. It is also worth noting, that Bison is compatible with flex, so one can directly tokenize the source code and then provide those tokens to Bison to do the Syntax and Semantic Analalysis. A typical Bison program consists of the following parts: \\
1. Definition part\\
Here are C-Definitions like data types, variables and functions defined. Also Bison definitions like tokens and non terminals are defined.\\
2. Grammar part\\
In that part one writes the grammar rules and also an action that will be executed when a specific rule is triggered. In this case a node for each action for the abstract syntax tree is created.\\
3. Optional C-Code\\
There one mostly writes the main function and other small functions that are used inside of the grammar rules or have been defined in the first part.\\
\begin{figure} [h]
\begin{center}
\centering
\includegraphics[width=16cm]{Bilder/Flex bison.png}
\caption{Interaction between Flex and Bison.}
\label{AST}
\end{center}
\end{figure}\\
\subsection{Optimization of the Abstract Syntax Tree}
Once the parser generated the abstract syntax tree, several traversals of the tree are taking place. Each aims to reduce the complexity of the tree, which in turn lowers the amount of time needed to execute the actions in the tree. Some prominent optimizations are constant folding, which sums up terms of constant values before executing the tree. Another optimization is dead code elimination, which removes non reachable subtrees of the Abstract Syntax Tree.
\subsection{Execution of the Abstract Syntax Tree}
In this step, after optimization took place, the tree is traversed for a final time and each nodesâ€˜ rule action is executed, which leads to the execution of the source file itself. In a final step, all nodes and auxiliar data structures are released and the memory given back to the CPU.
\begin{figure} [h]
\begin{center}
\centering
\includegraphics[width=16cm]{Bilder/out.png}
\caption{Example abstract syntax tree without any optimization.}
\label{AST}
\end{center}
\end{figure}\\

\section{Specifics of the language}
In this section the syntax of the language is presented along with its capabilities and limits.
\subsection{Data types}
The following data types are at the disposal of the language. A 16 bit Integer type with $ 2^{16} $ possible values. A floating point type Real, a string type, which is essentialy a C char array of fixed size and a dynically allocated Integer array. The Integer array can grow on runtime without the need of the programmer to reallocate memory. This happens internally. 
\subsection{Operations}
The following arithmetic and logical operations are possible. $ +, -, *, /, \%$ when using the real and integer data type. Also $ <, <=, >, >=, ==, !=, \&\&, ||, != $ can be used when working with Integer and Real types.
\subsection{Controll Flow}
The programmer can use while, for and ranged for loops in the following fashion: \\
int i; while(i < 10)\{ Instructions \}\\
int i; for(i = 0; i < 10; i++)\{ Instructions \}\\
int array [] arr; for(int i : arr)\{ Instructions \}\\
The programmer can use if else instructions in the following fashion:\\
if( condition )\{ Instrucions \} optional else \{ Instructions \}
\subsection{Functions and Scope handling}
The language is capable of executing functions. In the scope of the AST, functions are subtrees branching of the root node. Each root node of a function is stored in a global array, along with its call parameters. Functions can return exactly one arbitrary data type and take an arbitrary number of call parameters. Recursion is also possible. When entering a function, the current scope is hidden from the function scope and variables with the same identifier as a variable in the calling scope can be redeclared. When leaving the function, all variables of the function scope are deleted and the calling scope is entered again.

\section{Source Code}
The source code of the language and several source file containing examples like a multiplication table, fibonnaci series and guess my number along with the implementation of rule 110  \url{https://en.wikipedia.org/wiki/Rule_110} can be found in the following Github Repository: \url{https://github.com/FinrodFelegund/Compiler-Construction}\\
Of course the implementation of rule 110 does not show the turing completeness of my language, but rather the completeness of the C programming language.


\end{document}